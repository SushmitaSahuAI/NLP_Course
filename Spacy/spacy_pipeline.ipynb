{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ad32070-c696-4034-9ec4-1382e313c7e3",
   "metadata": {},
   "source": [
    "# Spacy pipeline\n",
    "When you call nlp on a text, spaCy first tokenizes the text to produce a Doc object. The Doc is then processed in several different steps – this is also referred to as the processing pipeline. The pipeline used by the trained pipelines typically include a tagger, a lemmatizer, a parser and an entity recognizer. Each pipeline component returns the processed Doc, which is then passed on to the next component.\n",
    "<img src = \"https://d33wubrfki0l68.cloudfront.net/3ad0582d97663a1272ffc4ccf09f1c5b335b17e9/7f49c/pipeline-fde48da9b43661abcdf62ab70a546d71.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b36de7-0c85-4165-833e-c5f6c3e690e1",
   "metadata": {},
   "source": [
    "## Built-in pipeline components\n",
    "<img src= \"https://i.stack.imgur.com/tPuUI.png\">\n",
    "\n",
    "The part-of-speech tagger sets the token.tag and token.pos attributes.\n",
    "\n",
    "The dependency parser adds the token.dep and token.head attributes and is also responsible for detecting sentences and base noun phrases, also known as noun chunks.\n",
    "\n",
    "The named entity recognizer adds the detected entities to the doc.ents property. It also sets entity type attributes on the tokens that indicate if a token is part of an entity or not.\n",
    "\n",
    "Finally, the text classifier sets category labels that apply to the whole text, and adds them to the doc.cats property.\n",
    "\n",
    "Because text categories are always very specific, the text classifier is not included in any of the pre-trained models by default. But you can use it to train your own system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b9c047-7ebe-4813-8632-0eceda3749a8",
   "metadata": {},
   "source": [
    "## Under the hood of any model\n",
    "- Pipeline is defined in model's meta.json in order\n",
    "- Built-in components need binary data to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bd2c5eb-254f-4e0c-9651-42039175cad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Load the installed model \"en_core_web_sm\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d516aaed-7d58-4d8e-943b-c28ca4c4a7ee",
   "metadata": {},
   "source": [
    "When we load a pipeline, Spacy first consults the meta.json and config.cfg. The config tells Spacy what language class to use, which components are in the pipeline, and how those components should be created.\n",
    "\n",
    "- Load the language class and data for the given ID via get_lang_class and initialize it. The Language class contains the shared vocabulary, tokenization rules, and language-specific settings.\n",
    "- Iterate over the pipeline names and look up each component name in the [components] block. The factory tells Spacy which component factory to use for adding the component with add_pipe. The settings are passed into the factory.\n",
    "- Make the model data available to the Language class by calling from_disk with the path to the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddc0142a-9cf4-487e-b830-4c4c38050848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5d1b90a-8284-44d2-91e3-645941b80a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7f98ed82f710>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7f98ed7d6650>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7f98ed98b3d0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7f98ed78f190>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7f98ed814410>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7f98ed98b1a0>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get list of names and components \n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54f34cb4-d679-4fa7-bf96-c61a48914d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.disable_pipes('tagger', 'parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbc42a30-8ef9-4c82-98f5-50768cb99d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5228dd6-d84e-47b8-acee-ba448e20b8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/spacy/pipeline/lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'This'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "/opt/conda/lib/python3.7/site-packages/spacy/pipeline/lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'sentence'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "/opt/conda/lib/python3.7/site-packages/spacy/pipeline/lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'would'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "/opt/conda/lib/python3.7/site-packages/spacy/pipeline/lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'n't'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "/opt/conda/lib/python3.7/site-packages/spacy/pipeline/lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'be'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "/opt/conda/lib/python3.7/site-packages/spacy/pipeline/lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'tagged'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "/opt/conda/lib/python3.7/site-packages/spacy/pipeline/lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'and'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n",
      "/opt/conda/lib/python3.7/site-packages/spacy/pipeline/lemmatizer.py:187: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for the token 'parsed'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108.format(text=string))\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\"])   # Loading the tagger and parser but don't enable them.\n",
    "doc = nlp(\"This sentence wouldn't be tagged and parsed\")\n",
    "\n",
    "nlp.enable_pipe(\"tagger\")     # Explicitly enabling the tagger later on.\n",
    "doc = nlp(\"This sentence will only be tagged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a0d500-f10f-43c7-8365-58a500e59395",
   "metadata": {},
   "source": [
    "************************************************************************\n",
    "We can use the nlp.select_pipes context manager to temporarily disable certain components for a given block. The select_pipes returns an object that lets us call its restore() method to restore the disabled components when needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12b6049f-17be-4c24-8284-e8bbd8e246e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "disabled = nlp.select_pipes(disable=[\"tagger\", \"parser\"])\n",
    "disabled.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4deb7e38-3024-49a4-aa03-c73d1662e340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7536570-6b13-4f34-951c-7885db452cb7",
   "metadata": {},
   "source": [
    "# Exclude Components\n",
    "In Spacy, we can also exclude a component by passing exclude keyword along with the list of excluded components. Unlike diable, it will not load the component and its data with the pipeline. Once the pipeline is loaded, there will be no reference to the excluded or include any components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e2d4fbd-3641-4ece-a8ea-7a27123224e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"ner\"])    # Load the pipeline without the entity recognizer\n",
    "doc = nlp(\"NER will be excluded from the pipeline\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f6e745-94de-4e0f-845d-6ccdb5e661ff",
   "metadata": {},
   "source": [
    "**************************************************\n",
    "We can also use the remove_pipe method to remove pipeline components from an existing pipeline, the rename_pipe method to rename them, or the replace_pipe method to replace them with a custom component entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7da8745-c326-4b43-975c-03d73b5209ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E001] No component 'ner' found in pipeline. Available names: ['tok2vec', 'tagger', 'senter', 'attribute_ruler', 'lemmatizer']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3053/4088240974.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ner\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"entityrecognizer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tagger\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"my_custom_tagger\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mrename_pipe\u001b[0;34m(self, old_name, new_name)\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mold_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m             raise ValueError(\n\u001b[0;32m--> 907\u001b[0;31m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mold_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m             )\n\u001b[1;32m    909\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E001] No component 'ner' found in pipeline. Available names: ['tok2vec', 'tagger', 'senter', 'attribute_ruler', 'lemmatizer']"
     ]
    }
   ],
   "source": [
    "nlp.remove_pipe(\"parser\")\n",
    "nlp.rename_pipe(\"ner\", \"entityrecognizer\")\n",
    "nlp.replace_pipe(\"tagger\", \"my_custom_tagger\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bf72ac8-eb42-4dc7-8bda-6a87cc24ecbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6e9ccd6-108d-451a-97b5-70e053f83aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'entityrecognizer']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.remove_pipe(\"parser\")\n",
    "nlp.rename_pipe(\"ner\", \"entityrecognizer\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae04d562-ed79-4720-9402-e2b001564421",
   "metadata": {},
   "source": [
    "# Adding Custom Attributes\n",
    "In Spacy, we can add metadata in the context and save it in custom attributes using nlp.pipe. This could be done by passing the text and its context in tuples form and passing a parameter astuples=True. The output will be a sequence of doc and context. In the example below, we are passing a list of texts along with some custom attributes to nlp.pipe and setting those attributes to the doc using doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f42217a0-94df-4f13-b466-d2e405756cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text1: This is the first text.\n",
      "text2: This is the second text.\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "if not Doc.has_extension(\"text_id\"):\n",
    "    Doc.set_extension(\"text_id\", default=None)\n",
    "text_tuples = [\n",
    "    (\"This is the first text.\", {\"text_id\": \"text1\"}),\n",
    "    (\"This is the second text.\", {\"text_id\": \"text2\"})\n",
    "]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc_tuples = nlp.pipe(text_tuples, as_tuples=True)\n",
    "\n",
    "docs = []\n",
    "for doc, context in doc_tuples:\n",
    "    doc._.text_id = context[\"text_id\"]\n",
    "    docs.append(doc)\n",
    "\n",
    "for doc in docs:\n",
    "    print(f\"{doc._.text_id}: {doc.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905741ba-02e0-4ccb-97a8-825b993af212",
   "metadata": {},
   "source": [
    "# Multiprocessing\n",
    "spaCy includes built-in support for multiprocessing with nlp.pipe using the n_process option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4667e9b8-aa8f-4581-a076-0007f01d7aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"This is a text\", \"These are lots of texts\", \"...\"]\n",
    "# Multiprocessing with 4 processes\n",
    "docs = nlp.pipe(texts, n_process=4)\n",
    "\n",
    "# With as many processes as CPUs (use with caution!)\n",
    "docs = nlp.pipe(texts, n_process=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f68a3b7-8a0a-4da3-8aca-860ba60b0624",
   "metadata": {},
   "source": [
    "# Analyzing Components\n",
    "In Spacy we can analyze the pipeline components using the nlp.analyze method which returns information about the components such as the attributes they set on the Doc and Token, whether they retokenize the Doc and which scores they produce during training. It will also show warnings if components require values that aren’t set by the previous component – for instance if the entity linker is used but no component that runs before it sets named entities. Setting pretty=True will pretty-print a table instead of only returning the structured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39d83621-9fa9-4704-8b4d-325199f8112f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output 1:\n",
      "{'summary': {'tagger': {'assigns': ['token.tag'], 'requires': [], 'scores': ['tag_acc'], 'retokenizes': False}, 'entity_linker': {'assigns': ['token.ent_kb_id'], 'requires': ['doc.ents', 'doc.sents', 'token.ent_iob', 'token.ent_type'], 'scores': ['nel_micro_f', 'nel_micro_r', 'nel_micro_p'], 'retokenizes': False}}, 'problems': {'tagger': [], 'entity_linker': ['doc.ents', 'doc.sents', 'token.ent_iob', 'token.ent_type']}, 'attrs': {'doc.sents': {'assigns': [], 'requires': ['entity_linker']}, 'token.ent_type': {'assigns': [], 'requires': ['entity_linker']}, 'doc.ents': {'assigns': [], 'requires': ['entity_linker']}, 'token.tag': {'assigns': ['tagger'], 'requires': []}, 'token.ent_iob': {'assigns': [], 'requires': ['entity_linker']}, 'token.ent_kb_id': {'assigns': ['entity_linker'], 'requires': []}}}\n",
      "\u001b[1m\n",
      "============================= Pipeline Overview =============================\u001b[0m\n",
      "\n",
      "#   Component       Assigns           Requires         Scores        Retokenizes\n",
      "-   -------------   ---------------   --------------   -----------   -----------\n",
      "0   tagger          token.tag                          tag_acc       False      \n",
      "                                                                                \n",
      "1   entity_linker   token.ent_kb_id   doc.ents         nel_micro_f   False      \n",
      "                                      doc.sents        nel_micro_r              \n",
      "                                      token.ent_iob    nel_micro_p              \n",
      "                                      token.ent_type                            \n",
      "\n",
      "\u001b[1m\n",
      "================================ Problems (4) ================================\u001b[0m\n",
      "\u001b[38;5;3m⚠ 'entity_linker' requirements not met: doc.ents, doc.sents,\n",
      "token.ent_iob, token.ent_type\u001b[0m\n",
      "Output 2:\n",
      "{'summary': {'tagger': {'assigns': ['token.tag'], 'requires': [], 'scores': ['tag_acc'], 'retokenizes': False}, 'entity_linker': {'assigns': ['token.ent_kb_id'], 'requires': ['doc.ents', 'doc.sents', 'token.ent_iob', 'token.ent_type'], 'scores': ['nel_micro_f', 'nel_micro_r', 'nel_micro_p'], 'retokenizes': False}}, 'problems': {'tagger': [], 'entity_linker': ['doc.ents', 'doc.sents', 'token.ent_iob', 'token.ent_type']}, 'attrs': {'doc.sents': {'assigns': [], 'requires': ['entity_linker']}, 'token.ent_type': {'assigns': [], 'requires': ['entity_linker']}, 'doc.ents': {'assigns': [], 'requires': ['entity_linker']}, 'token.tag': {'assigns': ['tagger'], 'requires': []}, 'token.ent_iob': {'assigns': [], 'requires': ['entity_linker']}, 'token.ent_kb_id': {'assigns': ['entity_linker'], 'requires': []}}}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"tagger\")\n",
    "# This is a problem because it needs entities and sentence boundaries\n",
    "nlp.add_pipe(\"entity_linker\")\n",
    "\n",
    "analysis = nlp.analyze_pipes()\n",
    "print(\"output 1:\")\n",
    "print(analysis)\n",
    "\n",
    "analysis = nlp.analyze_pipes(pretty=True)\n",
    "print(\"Output 2:\")\n",
    "print(analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb77b50-4704-4134-9b92-715ff0f2d80a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
