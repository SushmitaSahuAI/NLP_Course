{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaae912d-5f3f-400f-94d1-d2896c4de2db",
   "metadata": {},
   "source": [
    "# spaCy\n",
    "spaCy is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython. The library is published under the MIT license.\n",
    "\n",
    "It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning.\n",
    "\n",
    "## How different from nltk?\n",
    "NLTK was built with learning in mind. It is a great toolkit for teaching, learning, and experimenting with NLP. But spaCy was built with production-readiness in mind, focusing more on efficiency and performance.\n",
    "\n",
    "<img src = \"https://www.researchgate.net/profile/Aneek-Barman-Roy/publication/336147300/figure/fig4/AS:809035569852417@1569900519208/A-Comparison-of-NLTK-and-SpaCy-Frameworks-Bobriakov-2018.ppm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171a33a7-bb24-4b20-ae2e-9a8c44a1b818",
   "metadata": {},
   "source": [
    "# Piplelines\n",
    "Some spaCy's work independently , while some require trained pipelines.\n",
    "\n",
    "When nlp is called on a text, spacy first tokenizes the text to produce a Doc. Doc is then send through processing Pipleline. Pipleline consists of a tagger, parser, entity recognizer. Each pipeline component returns the processed Doc.\n",
    "<img src = \"https://spacy.io/pipeline-fde48da9b43661abcdf62ab70a546d71.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591635a0-19b8-488a-becf-6635b619b4d9",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "The central data structures in spaCy are the Language class, the Vocab and the Doc object. The Language class is used to process a text and turn it into a Doc object. It’s typically stored as a variable called nlp. The Doc object owns the sequence of tokens and all their annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cadf9fe-86b4-4b6f-b43b-c6ec2f09dbfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7ddbcf-8117-4d5f-b304-b4fcce56a873",
   "metadata": {},
   "source": [
    "******************************************************************************\n",
    "\n",
    "## nlp\n",
    "At the center of spacy is the object containing the processing pipeline. This variable is called nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b832bf-9145-44e7-8fbb-10ca6bdd71f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create English nlp object\n",
    "# Import the English language class\n",
    "from spacy.lang.en import English\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc608cc-bd9a-42ae-8f89-95731bf2da01",
   "metadata": {},
   "source": [
    "- nlp object can now be used as a function to analyse text\n",
    "- contains all different components in pipeline\n",
    "- supports language specific rules for tokenization(words and punctuations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9883c6db-fca4-4126-a959-ad1876ec27af",
   "metadata": {},
   "source": [
    "# Documents, tokens, spans\n",
    "Processing text with the nlp object returns a Doc object that holds all information about the tokens, their linguistic features and their relationships. Doc behaves like a normal python sequence.\n",
    "\n",
    "# Tokens\n",
    "Token objects represent tokens in a document, example a word or punctuation character \n",
    "<img src = \"https://hashouttech.com/static/3c5973a520b86c3660a9771453df5794/2bef9/span-object.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3f212f6-e973-4bff-afc5-b00c8e92fb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'text', '!']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"This is a text!\")\n",
    "# Token texts\n",
    "print ([token.text for token in doc])\n",
    "token_1 = doc[3]\n",
    "token_1.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaa5b38-b5a2-4c20-a686-71a42ca18320",
   "metadata": {},
   "source": [
    "# Spans\n",
    "Span object is the slice of the document consisting of one or more tokens. Its only a view of the doc, and doesnt contain any data in itself. Span indices are exclusive. So doc[2:4] is a span starting at token 2, up to but not including token 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c11e6606-811e-4b83-9ac9-68ca7ddc62a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a text'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span = doc[2:4]\n",
    "span.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378604e3-5e33-4a2c-8488-09b5e009bee0",
   "metadata": {},
   "source": [
    "## Creating span manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72c7c408-545e-4781-bbab-417d68db70b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "span.text :  New York\n",
      "span label :  GPE\n"
     ]
    }
   ],
   "source": [
    "# Import the Span object\n",
    "from spacy.tokens import Span\n",
    "# Create a Doc object\n",
    "doc = nlp(\"I live in New York\")\n",
    "# Span for \"New York\" with label GPE (geopolitical)\n",
    "span = Span(doc, 3, 5, label=\"GPE\")\n",
    "print (\"span.text : \",span.text)\n",
    "print (\"span label : \", span.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f7b31a-1c46-4d0e-9954-1fe552266443",
   "metadata": {},
   "source": [
    "## Lexical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b78de7e-647b-426e-85c0-1c15313bcaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:   [0, 1, 2, 3, 4, 5, 6]\n",
      "Text:   ['Does', 'this', 'toy', 'costs', '$', '10', '?']\n",
      "is_alpha:   [True, True, True, True, False, False, False]\n",
      "is_punct:   [False, False, False, False, False, False, True]\n",
      "like_num:   [False, False, False, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Does this toy costs $10 ?\")\n",
    "print (\"Index:  \",[token.i for token in doc])\n",
    "print (\"Text:  \",[token.text for token in doc])\n",
    "print (\"is_alpha:  \",[token.is_alpha for token in doc])\n",
    "print (\"is_punct:  \",[token.is_punct for token in doc])\n",
    "print (\"like_num:  \",[token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e43788-c196-4181-bcf5-a5cd9c956bc2",
   "metadata": {},
   "source": [
    "# Getting Started with spaCy\n",
    "There are two main components to spaCy\n",
    "- spaCy’s Statistical Models\n",
    "- spaCy’s Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7c67a7-3f99-426b-baa8-49ea7a582bfb",
   "metadata": {},
   "source": [
    "# Statistical Models\n",
    "Enables spaCy to predict linguistic attributes in context(example if a word is verb or and a span is a place)\n",
    "\n",
    "- Parts-of-speech tags\n",
    "- Syntactic dependencies\n",
    "- Named Enitities\n",
    "They are trained on large annotated texts. Can be further updated with more examples to fine-tune predictions\n",
    "\n",
    "# Model Packages\n",
    "spaCy provides many models which can be downloaded\n",
    "\n",
    "## Naming convention of model\n",
    "All pipeline packages follow the naming convention of [lang]_[name]\n",
    "\n",
    "- Type : Capabilities (e.g. core for general-purpose pipeline with tagging, parsing, lemmatization and named entity recognition, or dep for only tagging, parsing and lemmatization).\n",
    "- Genre: Type of text the pipeline is trained on, e.g. web or news.\n",
    "- Size: Package size indicator, sm, md, lg or trf (sm: no word vectors, md: reduced word vector table with 20k unique vectors for ~500k words, lg: large word vector table with ~500k entries)\n",
    "\n",
    "\"en_core_web_sm\" is a small model, trained on web-text and supports all core capabilities. The package provides binary weights that enable spaCy to make predictions. It also includes the vocab to tell spacy which language to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b24c591c-fd3f-47d4-a775-4ce077745f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
      "\u001b[1m\n",
      "================= Installed pipeline packages (spaCy v3.1.3) =================\u001b[0m\n",
      "\u001b[38;5;4mℹ spaCy installation: /opt/conda/lib/python3.7/site-packages/spacy\u001b[0m\n",
      "\n",
      "NAME             SPACY            VERSION                            \n",
      "en_core_web_lg   >=3.1.0,<3.2.0   \u001b[38;5;2m3.1.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "en_core_web_sm   >=3.1.0,<3.2.0   \u001b[38;5;2m3.1.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13cfa9c5-bbef-4032-bb93-1474e2fb84dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the installed model \"en_core_web_sm\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21adf35a-1b07-4cb2-9688-35c161d249ee",
   "metadata": {},
   "source": [
    "# Parts-of-speech Tags\n",
    "Along with POS we can predict how words are related. Example if a word subject or object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cc6b24e-aa16-4f8a-bd61-51edec7e0b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token.text: The; token.pos_: DET; token.dep_: det; token.head.text : kid\n",
      "token.text: kid; token.pos_: NOUN; token.dep_: nsubj; token.head.text : going\n",
      "token.text: is; token.pos_: AUX; token.dep_: aux; token.head.text : going\n",
      "token.text: going; token.pos_: VERB; token.dep_: ROOT; token.head.text : going\n",
      "token.text: to; token.pos_: ADP; token.dep_: prep; token.head.text : going\n",
      "token.text: school; token.pos_: NOUN; token.dep_: pobj; token.head.text : to\n",
      "token.text: .; token.pos_: PUNCT; token.dep_: punct; token.head.text : going\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"The kid is going to school.\")\n",
    "for token in doc:\n",
    "    print('token.text: {0}; token.pos_: {1}; token.dep_: {2}; token.head.text : {3}'.format(token.text,token.pos_, token.dep_, token.head.text))\n",
    "    # dep_ returns dependency label. head returns syntactic head attribute\n",
    "    # The head is the most important node in a phrase, while the Root is the most important node in the whole sentence: it is directly or indirectly the head of every other node."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
